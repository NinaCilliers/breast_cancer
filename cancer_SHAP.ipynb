{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><br></h3>\n",
    "<h3>What is game theory? </h3>\n",
    "Game theory is a branch of mathematics and economics that deals with the study of strategic decision-making in situations where the outcome of one individual's choices depends on the actions of others. It provides a framework to analyze interactions between rational agents and the strategies they employ to achieve their objectives. In the context of machine learning and explainable AI, SHAP values draw inspiration from game theory principles to attribute contributions to each feature in a prediction, allowing for a more interpretable and transparent understanding of the model's decision-making process.\n",
    "<h3><br><h3>\n",
    "<h3>What is a Shapley value?</h3>\n",
    "A SHAP value (SHapley Additive exPlanations) is a concept from cooperative game theory that has been adapted for use in machine learning and explainable artificial intelligence (AI). The idea is to attribute the prediction outcome to the individual features by considering all possible feature combinations and calculating their respective contributions. These contributions are then combined to provide a comprehensive and coherent explanation of the model's decision. This can explain predictions made by omplex models, such as machine learning models.\n",
    "<h3><br><h3>\n",
    "<h3>How are SHAP values used?</h3>\n",
    "\n",
    "1. **Model validation** - Understanding how a model works increases yser trust in model.\n",
    "2. **Debugging** - Interpretability allows for easier identification of errors.\n",
    "3. **Bias reduction** - Models that are used to make real-world decisions can be evaluated for bias. \n",
    "4. **Scientific understanding** - Key features can be identified for further exploration by \n",
    "scientists and researchers.\n",
    "5. **Local interpritability** - Factors influincing individual predictions can be examined.\n",
    "<h3><br><h3>\n",
    "<h3>What conditions must SHAP values satisfy?</h3>\n",
    "The Shapley value applies primarily in situations when the contributions of each actor are unequal, but they work in cooperation with each other to obtain the payoff.\n",
    "\n",
    "1. All the gains from cooperation are distributed among the playersâ€”none is wasted.\n",
    "2. Players that make equal contributions receive equal payoffs.\n",
    "3. The game cannot be divided into a set of smaller games that together achieve greater total gains\n",
    "4. A player that makes zero marginal contribution to the gains from cooperation receives zero payoff.\n",
    "<h3><br><h3>\n",
    "<h3>How do SHAP values differ from permutation importances?</h3>\n",
    "The main difference in methods is that permutation importance is based on the decrease in model performance, while SHAP importance is based on magnitude of feature attributions.Permutation importance is a simpler method that involves randomly shuffling the values of a single feature and measuring the resulting drop in model performance. The greater the drop in performance, the more important the feature is considered to be. Permutation importance does not offer local interpretabiity and is often less consistent.\n",
    "<h3><br><h3>\n",
    "<h3>How are SHAP values calculated?</h3>\n",
    "The equation to compute the Shapley value of featrure A is presented below with annotation, adapted from [original guide](http://adamlineberry.ai/shap/). F is the set of all features, so in this case F={A,B,C}. S is a subset of features. A single dash '\\' denotes 'without.' The fractional prefactor in the sum indicates the probability of a coalition, and the the term in pracets represent A's contribution to the colition.\n",
    "![]() <br>\n",
    "<br>\n",
    "\n",
    "<h3><br><h3>\n",
    "<h3>What is the computational cost of SHAP value determination?</h3>\n",
    "Calculation of a single SHAP value for one feature requires sampling many coalitions. This would make SHAP value calculations intractable on datasets with large amounts of features. Luckily aapproximate methods and optimizations are available, such as Python's SHAP, which will be used here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shap\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pickle.load(open('./test_train_cancer_data.pickle', 'rb'))\n",
    "X_train= _[0]\n",
    "y_train = _[1]\n",
    "X_test = _[2]\n",
    "y_test = _[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('./svc_cancer_model.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
